[
  {
    "objectID": "searchable_web_table.html",
    "href": "searchable_web_table.html",
    "title": "Making a searchable database table",
    "section": "",
    "text": "#### MAKING A SEARCHABLE TABLE FOR THE WEB ####\n\n# First -- let's talk about: what is the Internet? What IS a web page?\n# An oldie but a goodie from 2009: https://www.youtube.com/watch?v=7_LPdttKXPc\n# Bottom line, it's just a bunch of computers connected to each other. \n# A website? It's just files on someone else's computer (aka server)\n\n\n#We can use the \"DT\" package to easily make a sortable, filterable, searchable data table\n#Just this little bit of code does a whole lot - check it out:\n\nDT::datatable(events)\n\n\n\n\n\n\n\n#We can already sort, but what if we want to allow the table to be FILTERED too?\n#It's easy, we just add:\nDT::datatable(events, \n              rownames = FALSE, \n              filter = \"top\"# <--- NEW STUFF HERE\n              )\n\n\n\n\n\n\n\n#Now hmm, what's up with the filters on the text columns? Why aren't they working?\n#It's because of a quirk in DT tables where filters will only work on text that is converted to a factor\n#So let's do that\n\nevents <- events %>% \n  mutate(state = as_factor(state), event_type = as_factor(event_type))\n\n\n#Now let's try the DT table code again and see if it worked\nDT::datatable(events, \n              rownames = FALSE, \n              filter = \"top\")\n\n\n\n\n\n\n\n#Now, for the coup de gr?ce\n#let's add some buttons at the top of the page to let people copy, download, etc\n#we do that using a DT \"extenstion\" called, you guessed it, Buttons\n# https://rstudio.github.io/DT/extensions.html\n\nDT::datatable(events, \n              rownames = FALSE, \n              filter = \"top\", \n              extensions = 'Buttons', \n              options = list(   # <--- NEW STUFF STARTS HERE\n                dom = 'Bfrtip',\n                buttons = c('copy', 'csv', \"excel\")\n              )) %>%\n  DT::formatStyle('cand_lastname',  color = 'red', fontWeight = 'bold')\n\n\n\n\n\n\n\n## saving the result\n\n# first we just need to assign our table to a variable...\n\nmytable <- DT::datatable(events, \n                         rownames = FALSE, \n                         filter = \"top\", \n                         extensions = 'Buttons', \n                         options = list(\n                           dom = 'Bfrtip',\n                           buttons = c('copy', 'csv', \"excel\")\n                         )) %>%\n  DT::formatStyle('cand_lastname',  color = 'red', fontWeight = 'bold') \n\n# ... then just run this simple bit of code to export to html\nDT::saveWidget(mytable, \"mytable.html\")\n\n\n# We've now created a working web page that can be put anywhere on the internet we choose\n# Yay!\n\n# If we stay within the world of quarto though we don't need to export it, we can just display it\n# within the quarto page of course\n\nWhat if we have a little table and want a super minimal table with everything stripped out\n\nevents %>%\n  select(-description) %>%\n  head(5) %>% \n  DT::datatable(rownames = FALSE, \n                options = list(searching = FALSE, paging = FALSE, dom = \"tip\"))"
  },
  {
    "objectID": "01_virginia_election_project_datawrangling.html",
    "href": "01_virginia_election_project_datawrangling.html",
    "title": "Virginia Election Project",
    "section": "",
    "text": "Data available here: https://historical.elections.virginia.gov/elections/view/144567/\nA little column cleaning and we’ll load in the data file.\n\nprez_2020 <- read_csv(\"processed_data/va_2020_prez_cleaned.csv\")\n\nRows: 134 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): locality\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s see what we have\n\nhead(prez_2020) \n\n# A tibble: 6 × 4\n  locality         biden trump total_votes_2021_prez\n  <chr>            <dbl> <dbl>                 <dbl>\n1 Accomack County   7578  9172                 16962\n2 Albemarle County 42466 20804                 64657\n3 Alexandria City  66240 14544                 82508\n4 Alleghany County  2243  5859                  8203\n5 Amelia County     2411  5390                  7893\n6 Amherst County    5672 11041                 17005\n\n\nCalculating percentage of the vote\n\nprez_2020 %>% \n  mutate(\n    biden_pct = biden/total_votes_2021_prez\n  )\n\n# A tibble: 134 × 5\n   locality           biden trump total_votes_2021_prez biden_pct\n   <chr>              <dbl> <dbl>                 <dbl>     <dbl>\n 1 Accomack County     7578  9172                 16962     0.447\n 2 Albemarle County   42466 20804                 64657     0.657\n 3 Alexandria City    66240 14544                 82508     0.803\n 4 Alleghany County    2243  5859                  8203     0.273\n 5 Amelia County       2411  5390                  7893     0.305\n 6 Amherst County      5672 11041                 17005     0.334\n 7 Appomattox County   2418  6702                  9268     0.261\n 8 Arlington County  105344 22318                130699     0.806\n 9 Augusta County     10840 30714                 42278     0.256\n10 Bath County          646  1834                  2501     0.258\n# … with 124 more rows\n\n\nNow let’s do some rounding and move that decimal point\n\nprez_2020 %>% \n  mutate(\n    biden_pct = janitor::round_half_up(biden / total_votes_2021_prez * 100, 1)\n  )\n\n# A tibble: 134 × 5\n   locality           biden trump total_votes_2021_prez biden_pct\n   <chr>              <dbl> <dbl>                 <dbl>     <dbl>\n 1 Accomack County     7578  9172                 16962      44.7\n 2 Albemarle County   42466 20804                 64657      65.7\n 3 Alexandria City    66240 14544                 82508      80.3\n 4 Alleghany County    2243  5859                  8203      27.3\n 5 Amelia County       2411  5390                  7893      30.5\n 6 Amherst County      5672 11041                 17005      33.4\n 7 Appomattox County   2418  6702                  9268      26.1\n 8 Arlington County  105344 22318                130699      80.6\n 9 Augusta County     10840 30714                 42278      25.6\n10 Bath County          646  1834                  2501      25.8\n# … with 124 more rows\n\n\nNow trump too\n\nprez_2020 <- prez_2020 %>% \n  mutate(\n    biden_pct = janitor::round_half_up(biden / total_votes_2021_prez * 100, 2),\n    trump_pct = janitor::round_half_up(trump / total_votes_2021_prez * 100, 2)\n  )\n\nhead(prez_2020)\n\n# A tibble: 6 × 6\n  locality         biden trump total_votes_2021_prez biden_pct trump_pct\n  <chr>            <dbl> <dbl>                 <dbl>     <dbl>     <dbl>\n1 Accomack County   7578  9172                 16962      44.7      54.1\n2 Albemarle County 42466 20804                 64657      65.7      32.2\n3 Alexandria City  66240 14544                 82508      80.3      17.6\n4 Alleghany County  2243  5859                  8203      27.3      71.4\n5 Amelia County     2411  5390                  7893      30.6      68.3\n6 Amherst County    5672 11041                 17005      33.4      64.9"
  },
  {
    "objectID": "01_virginia_election_project_datawrangling.html#reshaping",
    "href": "01_virginia_election_project_datawrangling.html#reshaping",
    "title": "Virginia Election Project",
    "section": "Reshaping",
    "text": "Reshaping\nEnter pivot_wider().\nWe’ll get rid of everything we don’t need first.\n\ngov_2021 <- gov_2021 %>% \n  filter(ballot_name %in% c(\"Glenn A. Youngkin\", \"Terry R. McAuliffe\")) %>% \n  select(-locality_code,\n         -political_party)\n  \ngov_2021\n\n# A tibble: 266 × 4\n   locality_name    ballot_name        votes percentage\n   <chr>            <chr>              <int> <chr>     \n 1 ACCOMACK COUNTY  Glenn A. Youngkin   7878 61.08%    \n 2 ACCOMACK COUNTY  Terry R. McAuliffe  4948 38.37%    \n 3 ALBEMARLE COUNTY Glenn A. Youngkin  19141 37.21%    \n 4 ALBEMARLE COUNTY Terry R. McAuliffe 31919 62.05%    \n 5 ALEXANDRIA CITY  Glenn A. Youngkin  14013 24.02%    \n 6 ALEXANDRIA CITY  Terry R. McAuliffe 43866 75.20%    \n 7 ALLEGHANY COUNTY Glenn A. Youngkin   4530 74.52%    \n 8 ALLEGHANY COUNTY Terry R. McAuliffe  1518 24.97%    \n 9 AMELIA COUNTY    Glenn A. Youngkin   4720 74.19%    \n10 AMELIA COUNTY    Terry R. McAuliffe  1617 25.42%    \n# … with 256 more rows\n\n\nNow we’ll do the spreading out to reshape.\n\ngov_2021_wide <- gov_2021 %>% \n  pivot_wider(names_from = ballot_name, values_from = c(votes, percentage))\n\ngov_2021_wide\n\n# A tibble: 133 × 5\n   locality_name     `votes_Glenn A. Youngkin` votes_Terry R. …¹ perce…² perce…³\n   <chr>                                 <int>             <int> <chr>   <chr>  \n 1 ACCOMACK COUNTY                        7878              4948 61.08%  38.37% \n 2 ALBEMARLE COUNTY                      19141             31919 37.21%  62.05% \n 3 ALEXANDRIA CITY                       14013             43866 24.02%  75.20% \n 4 ALLEGHANY COUNTY                       4530              1518 74.52%  24.97% \n 5 AMELIA COUNTY                          4720              1617 74.19%  25.42% \n 6 AMHERST COUNTY                         9731              3897 71.00%  28.43% \n 7 APPOMATTOX COUNTY                      5971              1438 80.26%  19.33% \n 8 ARLINGTON COUNTY                      21548             73013 22.63%  76.67% \n 9 AUGUSTA COUNTY                        26196              7231 77.93%  21.51% \n10 BATH COUNTY                            1539               396 79.04%  20.34% \n# … with 123 more rows, and abbreviated variable names\n#   ¹​`votes_Terry R. McAuliffe`, ²​`percentage_Glenn A. Youngkin`,\n#   ³​`percentage_Terry R. McAuliffe`\n\n\nNice.\nThis is giving us some pretty long column names. we can change them after the fact using rename(). But first let’s clean the names to make it easier.\n\ngov_2021_wide <- gov_2021_wide %>% \n  clean_names()\n\nhead(gov_2021_wide)\n\n# A tibble: 6 × 5\n  locality_name    votes_glenn_a_youngkin votes_terry_r_mc_aul…¹ perce…² perce…³\n  <chr>                             <int>                  <int> <chr>   <chr>  \n1 ACCOMACK COUNTY                    7878                   4948 61.08%  38.37% \n2 ALBEMARLE COUNTY                  19141                  31919 37.21%  62.05% \n3 ALEXANDRIA CITY                   14013                  43866 24.02%  75.20% \n4 ALLEGHANY COUNTY                   4530                   1518 74.52%  24.97% \n5 AMELIA COUNTY                      4720                   1617 74.19%  25.42% \n6 AMHERST COUNTY                     9731                   3897 71.00%  28.43% \n# … with abbreviated variable names ¹​votes_terry_r_mc_auliffe,\n#   ²​percentage_glenn_a_youngkin, ³​percentage_terry_r_mc_auliffe\n\n\nNow let’s rename, and we’ll use similar names to what we had earlier in our 2021 results.\n\ngov_2021_wide <- gov_2021_wide %>% \n  rename(\n    youngkin = votes_glenn_a_youngkin,\n    mcauliffe = votes_terry_r_mc_auliffe,\n    pct_youngkin = percentage_glenn_a_youngkin,\n    pct_mcauliffe = percentage_terry_r_mc_auliffe\n  )\n\nhead(gov_2021_wide)\n\n# A tibble: 6 × 5\n  locality_name    youngkin mcauliffe pct_youngkin pct_mcauliffe\n  <chr>               <int>     <int> <chr>        <chr>        \n1 ACCOMACK COUNTY      7878      4948 61.08%       38.37%       \n2 ALBEMARLE COUNTY    19141     31919 37.21%       62.05%       \n3 ALEXANDRIA CITY     14013     43866 24.02%       75.20%       \n4 ALLEGHANY COUNTY     4530      1518 74.52%       24.97%       \n5 AMELIA COUNTY        4720      1617 74.19%       25.42%       \n6 AMHERST COUNTY       9731      3897 71.00%       28.43%       \n\n\nBingo.\nThere’s still one potential issue here. Can you see it?\nThe percentage columns are actually text values, not numbers. And they have that % sign in the text too. Let’s fix that using a handy function from the readr package, parse_number().\n\ngov_2021_wide <- gov_2021_wide %>% \n  mutate(\n    pct_youngkin = readr::parse_number(pct_youngkin),\n    pct_mcauliffe = readr::parse_number(pct_mcauliffe)\n  )\n\nhead(gov_2021_wide)\n\n# A tibble: 6 × 5\n  locality_name    youngkin mcauliffe pct_youngkin pct_mcauliffe\n  <chr>               <int>     <int>        <dbl>         <dbl>\n1 ACCOMACK COUNTY      7878      4948         61.1          38.4\n2 ALBEMARLE COUNTY    19141     31919         37.2          62.0\n3 ALEXANDRIA CITY     14013     43866         24.0          75.2\n4 ALLEGHANY COUNTY     4530      1518         74.5          25.0\n5 AMELIA COUNTY        4720      1617         74.2          25.4\n6 AMHERST COUNTY       9731      3897         71            28.4\n\n\nPerfect. Problem solved."
  },
  {
    "objectID": "01_virginia_election_project_datawrangling.html#comparing-gov-vs.-prez-results",
    "href": "01_virginia_election_project_datawrangling.html#comparing-gov-vs.-prez-results",
    "title": "Virginia Election Project",
    "section": "Comparing gov vs. prez results",
    "text": "Comparing gov vs. prez results\nNow that things are join, let’s actually go ahead and start making columns to compare the two elections and how the candidates did this time compared with last time.\nWhere should we go from here….?"
  },
  {
    "objectID": "roboticbees.html",
    "href": "roboticbees.html",
    "title": "The Buzz About Robotic Bees",
    "section": "",
    "text": "This article was published by Planet Forward on December 7th of 2022.\n------------------------------------------------------------\nIn front of hundreds of researchers at Harvard University, Mario Vallejo-Marin, Ph.D., a professor of biology and environmental science at the University of Sterling in Scotland, stood anxiously and gave an impassioned speech about the challenges he has faced in studying bee pollination. He was looking for solutions that can aid in the fight to protect ever-declining bee biodiversity, and he was open to trying anything. However, the last thing that Vallejo-Marin ever expected was to fly back to Scotland with the idea for a tiny robotic bee flying around in his head.\nAs Vallejo-Marin spoke to the Harvard scholars in late 2021 about his goal to use a vibrating source to conduct a controlled study on the characteristics of pollination in different species of bees, an eager hand shot up in the audience. The hand belonged to Noah Jafferis, Ph.D., a professor of electrical and computer engineering at the University of Massachusetts. Jafferis felt that he could help Vallejo-Marin achieve his goal by using a completely novel bio-robotic bee to replicate certain types of pollination in experiments. Vallejo-Marin’s first thought was one of skepticism, as the two researchers come from entirely different scientific backgrounds, but the more they talked, the more they realized that their interests seem to intertwine perfectly.\nIn March 2022, within months of meeting each other, Vallejo-Marin and Jafferis secured an $840,000 grant from the Human Frontier Science Program, an organization that promotes international interdisciplinary research related to living organisms.\n“This is one of the best pieces of news that you can receive as a scientist,” Vallejo-Marin said, with excitement in his voice. “Grant writing and getting funds in science is a very difficult process where there is a lot of chance, luck, and being in the right place at the right time.”\nVallejo-Marin’s biology lab in Scotland is filled with the deafening sound of buzzing, which shakes the entire room with high-pitched vibrations. The source of this buzzing becomes clear as Vallejo-Marin walks over to a large tank in the corner of the lab, looking affectionately at the hundreds of yellow and black insects flying and landing on artificial flowers. The focus of this lab is buzz pollination, which is a fairly rare and under-researched type of bee pollination that is vital to the health and fertilization of over 20,000 plant species and crops across the globe.\n“Buzz pollination is used on many different plant species, including a number of crops that humans rely on for food sources such as tomatoes and blueberries,” Vallejo-Marin said. Understanding how bees do it and which species do it could improve agriculture and help explain the importance of protecting rare species to maintain bee biodiversity."
  },
  {
    "objectID": "roboticbees.html#not-all-bees-buzz...-pollinate",
    "href": "roboticbees.html#not-all-bees-buzz...-pollinate",
    "title": "The Buzz About Robotic Bees",
    "section": "Not all bees buzz... pollinate",
    "text": "Not all bees buzz... pollinate\nOnly certain bees can buzz pollinate, and only certain plants can be pollinated in this way. Most bees pollinate by landing on flower petals and passively collecting pollen in the hairs on their bodies, which is then spread throughout the area as they fly. However, when the right kind of bee lands on the right kind of flower, the bee will contract the muscles in its thorax and start actively producing vibrations that make the entire flower shake. This motion causes the pollen grains inside to bounce back and forth and eventually shoot out of the flower.\n“You can see a shower, almost like a jet stream of pollen coming out of the flower,” Vallejo-Marin said. “This happens in a fraction of a second, and it takes less than a hundred milliseconds for the bees to shake free thousands and thousands of pollen grains.”\nThe main challenge that Vallejo-Marin has faced throughout his research is determining the morphology of buzz pollinators. This information would allow him to decipher what enables buzz pollinators to biologically pollinate in this way, compared to species of bees that cannot produce buzz vibrations. \n“We know very little about what makes a bee buzz a certain way, whether it’s size or speed or ecology or a combination of all of those factors,” Vallejo-Marin said. He has found that it is extremely difficult to replicate the incredibly fast vibrations that a such a tiny insect produces on a flower. This is where Dr. Noah Jafferis comes in, and this is where bee research will take a turn into the unknown."
  },
  {
    "objectID": "roboticbees.html#creating-the-prototype",
    "href": "roboticbees.html#creating-the-prototype",
    "title": "The Buzz About Robotic Bees",
    "section": "Creating the prototype",
    "text": "Creating the prototype\nJafferis has been interested in bio-inspired microrobots since he was in graduate school at Harvard University, where he helped develop a winged microrobot that was able to fly like a bee and simulate some of the aerodynamic aspects of insect flight. \n“The wings in those robo-bees were powered by piezoelectric actuators that bend back and forth, similar to the muscles that pull a bee’s wings back and forth,” Jafferis said. “I realized that these same muscles also produce the vibrations for buzz pollination, and my microrobots may be able to help in Mario’s research.”  \nPiezoelectric actuators are mechanical devices that take electrical energy and convert it directly into linear motion with high speed and force. Unlike larger robotic mechanisms, these actuators would be able to vibrate the body of the micro-robotic bee with extreme accuracy and precision while still maintaining the weight and size of an actual bee. Although the robo-bee pollinators are currently just prototypes, Jafferis is confident that he will be ready to begin the coding process in the near future. \nOver the next year, Vallejo-Marin will be using biomechanical equipment to analyze the buzz patterns of different bee species in his lab, even attempting to put tiny monitors directly on the bees to measure their movements during buzz pollination. \n“There is a variety of information that Mario will be measuring, such as the frequency and amplitude of the vibrations in buzz pollination, which I can plug directly into our robo-bees and tell that to vibrate with the same frequencies and amplitudes,” Jafferis said. \nBut, what can these robotic bees actually tell researchers that living bees cannot, and how would the data influence bee conservation and crop production? \nBio-inspired robots have been utilized to study the behavior and anatomy of many animal species in recent decades, and their use is only growing as they are able to provide novel information that scientists have never before been able to retrieve.\n“Biomimetic robots enable us to control individuals in experiments, and we can pinpoint differences in the behaviors of live animals, which we cannot do in any other setting,” said David Bierbach, a bio-robotics researcher, in a 2021 press conference at the University of Konstanz.\nJafferis believes that robotics could be the key to achieving detailed analyses of buzz pollination, illustrating how a bee’s characteristics affect their ability to buzz and what types of vibrations are most effective for pollination.\n“We can’t tell an actual bee, ‘Hey we want to see what happens if you let go of one of your feet,’ but with a robot we can,” Jafferis said. “We can tell the robots to vibrate and grab flowers and do things in different ways that a bee is not doing on its own.”"
  },
  {
    "objectID": "roboticbees.html#the-need-for-bees",
    "href": "roboticbees.html#the-need-for-bees",
    "title": "The Buzz About Robotic Bees",
    "section": "The need for bees",
    "text": "The need for bees\nBees are in a current state of worldwide decline, and their biodiversity is suffering in a way that may lead to the complete extinction of certain rare species in the near future.  \n“We fear bee decline,” said Michael Roswell, Ph.D., an entomologist at the University of Maryland. “With 4,000 species of bees in the United States and 20,000 species on earth, we expect that some will be doing better than others at any given time, but we’re certainly afraid that many rare species are doing worse and worse.”\nRoswell published a study in April 2022 that highlighted the importance of rare bees in plant-pollinator networks. He and his team were able to show that less common species, many of which were buzz pollinators, often fertilize plants that more common species do not pollinate at all. Roswell believes that Vallejo-Marin and Jafferis’ study would expand his research in a way that could help to target bee conservation efforts towards the buzz pollinators that are most in need of protection and that are needed most by their ecological environments.\n“We are trying to use this project as a platform to help people realize that not all bees are the same and that different bees do different jobs,” Vallejo-Marin said. “It is important to maintain the biodiversity of bees so that not just one or two species are doing well, because every species is vitally important.”\nIf the robotic bees are able to successfully replicate buzz pollination, they may be able to aid in sustainable methods of crop production as well.   \n“If one bee species is suffering, the robots could tell us if there is another species that can fill in and pollinate these vital crops instead, or if we need to focus all of our resources on the conservation of that species,” Jafferis said.\nThe researchers are optimistic that they will eventually be able to pair the optimal bee with the optimal crop, which would not only supply humans with an effective and environmentally conscious method of crop fertilization, but would also help bees to strengthen their populations in areas with bounties of plants that match their pollination styles.\nOver the next three years, Vallejo-Marin and Jafferis will be working toward these sizable goals, combining their expertise to create a groundbreaking interdisciplinary device that may be able to positively influence bees, humans, plants, and the environment as a whole.  \n“It is hard to work with bees and not fall in love with them,” Vallejo-Marin said. “They are such charismatic creatures, and once you care about them you will care about them forever.”\nTags: Biodiversity,  Bees,  ecology,  animals,  research,  pollinators,  nature, biotechnology"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Emma Saaty",
    "section": "",
    "text": "Hi, I’m Emma and I am a Biological Anthropology and Science Journalism student at George Washington University."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Emma Saaty",
    "section": "Education",
    "text": "Education\nGeorge Washington University, GPA: 4.0, Honors: Presidential Scholarship, Dean’s List\nMcLean High School, GPA: 4.0, Activities: Choir, National Honor Society"
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "Emma Saaty",
    "section": "Work Experience",
    "text": "Work Experience\nGWU Undergraduate Researcher: Center for the Advanced Study of Human Paleobiology, Skeletal Biology Lab\nSmithsonian Institution Center for Folklife and Cultural Heritage Intern\nLee-Fendall House Collections Management Intern"
  },
  {
    "objectID": "index.html#resume",
    "href": "index.html#resume",
    "title": "Emma Saaty",
    "section": "Resume",
    "text": "Resume"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Created: 12/3/2022\nClick the “Home” tab for information about Emma’s work experience and education.\nClick the “Portfolio” tab for examples of Emma’s work as a science journalist and biological anthropology student."
  },
  {
    "objectID": "electionwalkthrough.html",
    "href": "electionwalkthrough.html",
    "title": "Virginia Election Project",
    "section": "",
    "text": "First, I am creating an interactive data table to explore the data from the two elections. I am using the DT::datatable() function to achieve this, and putting the data (joined_comparison) in the parentheses. You will see that the interactive table allows you to filter each column from lowest to highest and search for key terms and counties.\n\nDT:: datatable(joined_vacomparison)\n\n\n\n\n\n\nI am now creating a column in the dataset showing the percentage of votes that republican Governor candidate Youngkin received compared to the percent of votes that republican Presidential candidate Trump received. I am doing this with the use of the mutate() function, and creating a new name for the column and making it equal to the pct_youngkin column minus the trump_pct column. This will give me a column in the main dataset entitled youngkin_minus_trump.\nWithin this same code chunk, I can also create a table that will give me only the data the I want to include in a chart to visualize this percent difference. I will use the select () function to get only the locality, pct_youngkin, trump_pct, and my new column youngkin_minus_trump. I can use the arrange () function and make the percentage difference column arrange from highest difference to lowest difference. As I only wish to show the 5 localities with the highest differences in percentages between Youngkin and Trump, I will use the head () function to give me only the first 5 rows in this table.\n\nyoungkin_over_trump <- joined_vacomparison %>%\n  mutate(youngkin_minus_trump = pct_youngkin - trump_pct) %>%\n  select(locality, pct_youngkin, trump_pct, youngkin_minus_trump) %>%\n  arrange(desc(youngkin_minus_trump)) %>%\n  head(5)\n\n\nDT::datatable(youngkin_over_trump)\n\n\n\n\n\n\nI will now be creating a chart with my new simplified data table to show the 5 counties that have the highest difference between Youngkin and Trump’s vote percentages. I will start off by using the ggplot package and putting in the new youngkin_over_trump dataset. I will then use the aesthetic (aes) function to put in the locality column as my X value and the youngkin_minus_trump column as my Y value. I will then use the geom_col () function to make it a bar chart, and I will choose a color for the border and the fill of the bars.\nI will use the scale_y_continuous () and the scale_x_discrete () functions to create the axis labels names and make sure that they are formatted correctly. I will then use the theme () function to adjust the tilt of the county labels to make sure that they do not overlap with each other on the X axis. This will then create a bar chart that shows me from lowest to highest, the top 5 counties that had the highest differences in votes between Youngkin and Trump.\n\nggplot(youngkin_over_trump, aes (x = reorder(locality, youngkin_minus_trump), y = youngkin_minus_trump))+\n  geom_col(color = \"black\", fill = \"#9ebcda\")+\n  scale_y_continuous(name = \"Difference Between Youngkin and Trump's Votes\")+\n  scale_x_discrete(name = \"Counties\")+\ntheme(axis.text.x = element_text(angle = 10))\n\n\n\n\nNext I will be creating a similar chart that shows the 5 counties where Youngkin got the highest percentage of the votes. I will first create a new dataset entitled highest_youngkin_percent out of the old joined_vacomparison dataset. I will use the arrange () function to put the highest percentages at the top of the column, and the head () function to include only the first 5 rows.\nI will then use the same ggplot function as the above chart and all of the same functions, except I will use different border and fill colors, and create new names for the axis labels. I will also exclude the theme () function, as the names of these counties fit without needing to tilt them at all.\n\nhighest_youngkin_pct <- joined_vacomparison %>%\n  arrange(desc(pct_youngkin)) %>%\n  head(5)\n\nggplot(highest_youngkin_pct, aes (x = reorder(locality, pct_youngkin), y = pct_youngkin))+\n  geom_col(color = \"yellow\", fill = \"dark green\")+\n  scale_y_continuous(name = \"Youngkin Vote Percentage\")+\n  scale_x_discrete(name = \"Counties\")\n\n\n\n\nI will now be creating a table that shows which counties democratic Governor candidate Mcauliffe got the highest percentage of the votes. I will first be creating a name for a new dataset, entitled highest_mcauliffe_pct out of the old joined_vacomparison dataset. I will then use the select () function to pull only the locality and the pct_mcauliffe columns out of the original data. I will then use the arrange () function to put the pct_mcauliffe column in descending order from highest to lowest. The head () function allows me to keep only the top 5 rows. I will finish by creating an interactive datatable using the DT:: datatable () function and putting in the name of my new dataset.\n\nhighest_mcauliffe_pct <- joined_vacomparison %>%\n  select(locality, pct_mcauliffe) %>%\n  arrange(desc(pct_mcauliffe)) %>%\n  head(5)\n\n\nDT::datatable(highest_mcauliffe_pct)\n\n\n\n\n\n\nI will now be looking at the top 5 counties that were Youngkin’s biggest losses. I will start by creating a new dataset entitled biggest_youngkin_losses from the original joined_vacomparison dataset. I will then use the mutate () function to create a new column that shows Youngkin’s vote percentage minus Mcauliffe’s vote percentage. I will select only the locality, both percentage columns and the new youngkin_minus_mcauliffe column. I will then arrange the new column in ascending order, with the smallest negative numbers on the top, because that will show where Youngkin had the lowest votes compared to Mcauliffe. I will use the head () function to take only the top 5 rows. I will then create the interactive datatable using the DT:: datatable () function and putting in the name of my new dataset.\n\nbiggest_youngkin_losses <- joined_vacomparison %>%\n  mutate(youngkin_minus_mcauliffe = pct_youngkin - pct_mcauliffe) %>%\n  select(locality, pct_youngkin, pct_mcauliffe, youngkin_minus_mcauliffe) %>%\n  arrange(youngkin_minus_mcauliffe) %>%\n  head(5)\n\nDT::datatable(biggest_youngkin_losses)\n\n\n\n\n\n\nI will also be looking at the biggest losses for Youngkin using a chart, in which I will follow all of the same steps as before, using the ggplot function. I will make the X axis the counties, and the Y axis the new column that I created for the lowest 5 percentages of youngkin’s votes minus mcauliffe’s votes. I will choose two new colors for the border and the fill of the bars, and I will add axis label names. I will use the theme () function to tilt the county names to make them all fit at the bottom.\n\nggplot(biggest_youngkin_losses, aes (x = reorder(locality, youngkin_minus_mcauliffe), y = youngkin_minus_mcauliffe))+\n  geom_col(color = \"red\", fill = \"purple\")+\n  scale_y_reverse(name = \"Youngkin's Margin of Loss to Mcauliffe\")+\n  scale_x_discrete(name = \"Counties\")+\ntheme(axis.text.x = element_text(angle = 8))\n\n\n\n\nNext I will be comparing President Biden’s votes to Governor Mcauliffe’s votes to see were Biden won a higher percentage of votes than Mcauliffe did. I will start by creating a new dataset entitled biden_over_mcauliffe from the original joined_vacomparison dataset. I will then use the mutate () function to create a new column that shows Biden’s vote percentage minus Mcauliffe’s vote percentage. I will select only the locality, both percentage columns and the new Biden_minus_mcauliffe column. I will then arrange the new column in descending order, with the largest numbers on the top, because that will show where Biden had a larger percentage of the votes than Mcauliffe in each county. I will use the head () function to take only the top 5 rows. I will then create the interactive datatable using the DT:: datatable () function and putting in the name of my new dataset.\n\nbiden_over_mcauliffe <- joined_vacomparison %>%\n  mutate(biden_minus_mcauliffe = biden_pct - pct_mcauliffe) %>%\n  select(locality, biden_pct, pct_mcauliffe, biden_minus_mcauliffe) %>%\n  arrange(desc(biden_minus_mcauliffe)) %>%\n  head(5)\n\nDT::datatable(biden_over_mcauliffe)\n\n\n\n\n\n\nI will also be looking at the Biden’s percentages compared to Mcauliffe’s using a chart, in which I will follow all of the same steps as before, using the ggplot function. I will make the X axis the counties, and the Y axis the new column that I created for the 5 counties with the highest percent differences of Biden winning more votes than Mcauliffe. I will choose two new colors for the border and the fill of the bars, and I will add axis label names. I will use the theme () function to tilt the county names to make them all fit at the bottom.\n\nggplot(biden_over_mcauliffe, aes (x = reorder(locality, biden_minus_mcauliffe), y = biden_minus_mcauliffe))+\n  geom_col(color = \"green\", fill = \"dark blue\")+\n  scale_y_continuous(name = \"Percent of Biden's Votes over Mcauliffe's\")+\n  scale_x_discrete(name = \"Localities\")+\n  theme(axis.text.x = element_text(angle = 10))\n\n\n\n\nFinally, I will be creating a table that shows the total democrat percentages vs the total republican percentages. I will start by creating a new dataset entitled republican_vs_democrat_totals from the original joined_vacomparison dataset. I will then use the mutate () function to create a new column that adds biden and mcauliffe’s percentages together. I will also use the mutate () function again to create a new column that adds trump and youngkin’s percentages together. I will then select only the locality, and both new total columns. This will give me a table that compares the total democrat percentages and the total republican percentages by counties. I will then create the interactive datatable using the DT:: datatable () function and putting in the name of my new dataset. This table could be useful in figuring out with counties lean more heavily democrat or republican overall.\n\nrepublican_vs_democrat_totals <- joined_vacomparison %>%\n  mutate(democrat_total = biden_pct + pct_mcauliffe) %>%\n  mutate(republican_total = trump_pct + pct_youngkin) %>%\n  select(locality, republican_total, democrat_total)\n\nDT::datatable(republican_vs_democrat_totals)"
  },
  {
    "objectID": "Portfolio.html",
    "href": "Portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "The Buzz About Robotic Bees, 12/2/2022\nFeature Published by Planet Forward\nSummary: Robotic bees are being developed to study buzz pollination and help support the conservation of declining bee populations across the globe."
  },
  {
    "objectID": "Portfolio.html#c.a.r.t.-archaeology-article",
    "href": "Portfolio.html#c.a.r.t.-archaeology-article",
    "title": "Portfolio",
    "section": "C.A.R.T. Archaeology Article",
    "text": "C.A.R.T. Archaeology Article\nNorth American Gray Stoneware, 12/1/2018\nArticle Published by C.A.R.T. Archaeology Blog\nSummary: North American Stoneware was first produced in the English colonies of North America in the early 1700s, and was heavily influenced by British and German traditions."
  },
  {
    "objectID": "Portfolio.html#briana-pobiner-profile-article",
    "href": "Portfolio.html#briana-pobiner-profile-article",
    "title": "Portfolio",
    "section": "Briana Pobiner Profile Article",
    "text": "Briana Pobiner Profile Article\nBriana Pobiner: Breaking Ground in More Ways Than One\nArticle written for science journalism course at GW, 4/12/2022\nSummary: Exploring the life and career of National Museum of Natural History scientist Briana Pobiner, as she works towards the goal of changing the future by uncovering the past."
  },
  {
    "objectID": "epigeneticspaper.html",
    "href": "epigeneticspaper.html",
    "title": "Epigenetic Testing and Preterm Birth Paper",
    "section": "",
    "text": "This article was written for my science journalism course in the spring of 2022. Preterm birth has been a leading concern in the medical world for decades, and a genetic research team may have unlocked a key biological factor that could help to predict and prevent preterm births in the future. I interviewed the principal investigator on the study, along with another expert on infant health, and a father who has been touched by the effects of preterm birth. This article will teach you about the biology behind preterm birth and the impact that genetic research can have on this issue across the globe."
  },
  {
    "objectID": "epigeneticspaper.html#preventing-preterm-birth-may-be-possible-with-new-epigenetic-test",
    "href": "epigeneticspaper.html#preventing-preterm-birth-may-be-possible-with-new-epigenetic-test",
    "title": "Epigenetic Testing and Preterm Birth Paper",
    "section": "Preventing Preterm Birth may be Possible with New Epigenetic Test",
    "text": "Preventing Preterm Birth may be Possible with New Epigenetic Test\nOver 100 epigenetic biomarkers have been found to predict preterm birth susceptibility, which could lead to the early detection and possible prevention of preterm births and the health impacts that often accompany them, a new study finds.\nPreterm birth is a major cause of infant mortality, affecting one out of every 10 live births across the globe.  Researchers are hopeful that epigenetic testing may be the key to diagnosing and treating these high risk births before they happen in the early stages of pregnancy, according to a study published on March 1 in Nature’s Scientific Reports.\n“If you think about all of the genes in your DNA, what turns them on and off isn’t the DNA sequence, but it’s the molecular structure around your genes known as epigenetics,” says Dr. Michael Skinner, a lead researcher on the study and a professor of biology at Washington State University.  “These chemical markers are equally if not more important than genetics in the regulation of biological processes like reproduction.” \nIn this study, researchers took cheek swabs from 40 mother, father, and child groups shortly after the children were born.  They compared the DNA of those who had preterm births to those who carried to full term, and found that there were distinct epigenetic differences between the two groups.  Each member of the family groups who experienced preterm births had very similar epigenetic mutations, indicating that these biomarkers can be passed between generations. \n“If your mother had a premature birth, you will have a susceptibility to have a premature birth, and you will pass it on to your own daughter as well,” says Skinner, adding that epigenetic traits have been observed to be inherited for hundreds of generations in other organisms. \nAlthough mothers and daughters had the most similarities in their biomarker regions, fathers were also found to contribute to premature birth susceptibility.  Numerous environmental toxins, diet variations, and other forms of parental early life exposure could be driving these epigenetic inheritances, and determining these risk factors is next on Skinner’s list of research topics relating to his biomarker findings. \nThe ultimate goal of this study was to develop a simple and widely available epigenetic test that would be able to detect preterm birth and reduce adverse effects.  Previous research shows that children who are born preterm have a higher likelihood of developing cognitive disabilities and physiological impairments in the future.\n“Preterm birth is incredibly frequent and costly to families and society in general,” says Dr. Gary Shaw, a principal investigator at the March of Dimes Prematurity Research Center at Stanford University.  “The epigenome is absolutely a great place for these researchers to be looking, however all of this will have to be verified in larger and more clinically robust trials before testing becomes broadly available.”\nThere are already numerous medical interventions and obstetrical management methods available to treat and prevent preterm birth that are currently underutilized, as parents and healthcare workers are largely unaware of preterm risk.\n“The state of maternal health in this country is not fine,” Charles Johnson, a father, told March of Dimes after he lost his preterm daughter to complications at birth. “People walking into what they expect to be the happiest day of their lives and not living to raise their children, or their babies not coming home—it’s not fine.”\nThe ability to epigenetically test for susceptibility in the first trimester of pregnancy would have profound impacts on society, allowing for more effective healthcare practices and increasing infant survival rates.  While there are numerous factors that still need to be studied before these tests can be implemented throughout clinical settings, this research provides a promising starting point.\n“We won’t necessarily be able to fix the problem of preterm birth,” says Skinner, “but I am confident that these epigenetic biomarkers will enable us to treat it.”"
  },
  {
    "objectID": "walkthrough.html",
    "href": "walkthrough.html",
    "title": "Analysis Walkthrough",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(lubridate)\n\n\n\nAttaching package: 'lubridate'\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\nRun this line of code to load the data for the analysis\n\n\nCode\nhousesales <- ggplot2::txhousing\n\n\nLook at the data set\n\n\nCode\nhousesales\n\n\n# A tibble: 8,602 × 9\n   city     year month sales   volume median listings inventory  date\n   <chr>   <int> <int> <dbl>    <dbl>  <dbl>    <dbl>     <dbl> <dbl>\n 1 Abilene  2000     1    72  5380000  71400      701       6.3 2000 \n 2 Abilene  2000     2    98  6505000  58700      746       6.6 2000.\n 3 Abilene  2000     3   130  9285000  58100      784       6.8 2000.\n 4 Abilene  2000     4    98  9730000  68600      785       6.9 2000.\n 5 Abilene  2000     5   141 10590000  67300      794       6.8 2000.\n 6 Abilene  2000     6   156 13910000  66900      780       6.6 2000.\n 7 Abilene  2000     7   152 12635000  73500      742       6.2 2000.\n 8 Abilene  2000     8   131 10710000  75000      765       6.4 2001.\n 9 Abilene  2000     9   104  7615000  64500      771       6.5 2001.\n10 Abilene  2000    10   101  7040000  59300      764       6.6 2001.\n# … with 8,592 more rows\n\n\nFilter to return only results from 2015 by putting the house sale data set title followed by a pipe and then filter by the column year and 2015.\n\n\nCode\nhousesales %>%\n  filter(year == 2015)\n\n\n# A tibble: 322 × 9\n   city      year month sales   volume median listings inventory  date\n   <chr>    <int> <int> <dbl>    <dbl>  <dbl>    <dbl>     <dbl> <dbl>\n 1 Abilene   2015     1   158 23486998 134100      801       4.4 2015 \n 2 Abilene   2015     2   151 19834263 126500      767       4.1 2015.\n 3 Abilene   2015     3   198 31869437 136800      821       4.4 2015.\n 4 Abilene   2015     4   201 28301159 129600      891       4.7 2015.\n 5 Abilene   2015     5   199 31385757 144700      919       4.8 2015.\n 6 Abilene   2015     6   260 41396230 141500      965       5   2015.\n 7 Abilene   2015     7   268 45845730 148700      986       5   2016.\n 8 Amarillo  2015     1   204 33188726 138500     1120       4.3 2015 \n 9 Amarillo  2015     2   188 34355428 149400     1084       4.2 2015.\n10 Amarillo  2015     3   317 53603130 140900     1051       3.9 2015.\n# … with 312 more rows\n\n\nFilter the results from 2010 on by filtering the column year with a greater than or equal sign for 2010. This will give you all of the data from 2010-2015.\n\n\nCode\nhousesales %>%\n  filter(year >= 2010)\n\n\n# A tibble: 3,082 × 9\n   city     year month sales   volume median listings inventory  date\n   <chr>   <int> <int> <dbl>    <dbl>  <dbl>    <dbl>     <dbl> <dbl>\n 1 Abilene  2010     1    73  9130783 112200      868       6.4 2010 \n 2 Abilene  2010     2    93 10372904  98300      830       6.1 2010.\n 3 Abilene  2010     3   133 16517713 114000      854       6.3 2010.\n 4 Abilene  2010     4   161 18788002 103600      859       6.3 2010.\n 5 Abilene  2010     5   200 22804393  99300      914       6.5 2010.\n 6 Abilene  2010     6   169 23216943 127900      932       6.7 2010.\n 7 Abilene  2010     7   159 22363123 127300      915       6.6 2010.\n 8 Abilene  2010     8   144 17504580 122000      936       6.7 2011.\n 9 Abilene  2010     9   116 15475763 121300      899       6.5 2011.\n10 Abilene  2010    10   111 14570529 111900      863       6.4 2011.\n# … with 3,072 more rows\n\n\nThe “city” column contains the relevant city. Filter to return only results for Houston. Put in the housesale dataset title followed by a pipe then filter by the column city with double equals sign for “Houston”, which will give you all of the data with Houston as the city.\n\n\nCode\nhousesales %>%\n  filter(city == \"Houston\")\n\n\n# A tibble: 187 × 9\n   city     year month sales    volume median listings inventory  date\n   <chr>   <int> <int> <dbl>     <dbl>  <dbl>    <dbl>     <dbl> <dbl>\n 1 Houston  2000     1  2653 381805283 102500    16768       3.9 2000 \n 2 Houston  2000     2  3687 536456803 110300    16933       3.9 2000.\n 3 Houston  2000     3  4733 709112659 109500    17058       3.9 2000.\n 4 Houston  2000     4  4364 649712779 110800    17716       4.1 2000.\n 5 Houston  2000     5  5215 809459231 112700    18461       4.2 2000.\n 6 Houston  2000     6  5655 887396592 117900    18959       4.3 2000.\n 7 Houston  2000     7  5009 770284031 118300    19391       4.4 2000.\n 8 Houston  2000     8  5134 800032008 114600    19534       4.5 2001.\n 9 Houston  2000     9  4262 671716256 115900    19558       4.5 2001.\n10 Houston  2000    10  4068 621419904 112400    19752       4.5 2001.\n# … with 177 more rows\n\n\nFilter for only where the city is Houston as above, and now also filter for only the year 2010. The results should give you 12 records, one for each month in 2010 for Houston. Put in the housesale dataset title followed by a pipe then filter by the column city with double equals sign for “Houston”. Separate with a comma and then filter by the column year for 2010.\n\n\nCode\nhousesales %>%\n  filter(city == \"Houston\" , year == 2010)\n\n\n# A tibble: 12 × 9\n   city     year month sales     volume median listings inventory  date\n   <chr>   <int> <int> <dbl>      <dbl>  <dbl>    <dbl>     <dbl> <dbl>\n 1 Houston  2010     1  2856  544338012 142300    31581       6.3 2010 \n 2 Houston  2010     2  3684  731357757 146500    32695       6.6 2010.\n 3 Houston  2010     3  5389 1116539769 152300    34880       6.9 2010.\n 4 Houston  2010     4  5932 1191105918 151800    34555       6.7 2010.\n 5 Houston  2010     5  6335 1289709166 152500    36724       7   2010.\n 6 Houston  2010     6  6117 1324315238 156800    39303       7.5 2010.\n 7 Houston  2010     7  4719 1028687548 157100    40409       7.9 2010.\n 8 Houston  2010     8  4691  994092936 156900    40154       8   2011.\n 9 Houston  2010     9  4355  915074585 153200    39159       7.9 2011.\n10 Houston  2010    10  4075  824541611 148400    38114       7.9 2011.\n11 Houston  2010    11  3924  842238262 150100    37031       7.8 2011.\n12 Houston  2010    12  4730 1028424811 157500    34971       7.4 2011.\n\n\nBuild on what you’ve done above. Filter for only where the city is Houston as above, and the year is 2010. Now add a sort using arrange() to sort the results based on the number of home sales (the “sales” column) from most to least. Put in the housesale dataset title followed by a pipe then filter by the column city with double equals sign for “Houston”. Separate with a comma and then filter by the column year for 2010, followed by a pipe. Use the arrange command with (desc) in parentheses to arrange by the sales column.\n\n\nCode\nhousesales %>%\n  filter(city == \"Houston\" , year == 2010) %>%\n  arrange(desc(sales))\n\n\n# A tibble: 12 × 9\n   city     year month sales     volume median listings inventory  date\n   <chr>   <int> <int> <dbl>      <dbl>  <dbl>    <dbl>     <dbl> <dbl>\n 1 Houston  2010     5  6335 1289709166 152500    36724       7   2010.\n 2 Houston  2010     6  6117 1324315238 156800    39303       7.5 2010.\n 3 Houston  2010     4  5932 1191105918 151800    34555       6.7 2010.\n 4 Houston  2010     3  5389 1116539769 152300    34880       6.9 2010.\n 5 Houston  2010    12  4730 1028424811 157500    34971       7.4 2011.\n 6 Houston  2010     7  4719 1028687548 157100    40409       7.9 2010.\n 7 Houston  2010     8  4691  994092936 156900    40154       8   2011.\n 8 Houston  2010     9  4355  915074585 153200    39159       7.9 2011.\n 9 Houston  2010    10  4075  824541611 148400    38114       7.9 2011.\n10 Houston  2010    11  3924  842238262 150100    37031       7.8 2011.\n11 Houston  2010     2  3684  731357757 146500    32695       6.6 2010.\n12 Houston  2010     1  2856  544338012 142300    31581       6.3 2010 \n\n\nPiggyback on what you’ve done above. Do the same as before, only this time instead of Houston return records for Dallas. Put in the housesale dataset title followed by a pipe then filter by the column city with double equals sign for “Dallas”. Separate with a comma and then filter by the column year for 2010, followed by a pipe. Use the arrange command with (desc) in parentheses to arrange by the sales column.\n\n\nCode\nhousesales %>%\n  filter(city == \"Dallas\" , year == 2010) %>%\n  arrange(desc(sales))\n\n\n# A tibble: 12 × 9\n   city    year month sales     volume median listings inventory  date\n   <chr>  <int> <int> <dbl>      <dbl>  <dbl>    <dbl>     <dbl> <dbl>\n 1 Dallas  2010     5  4891 1025677623 161900    25680       6.4 2010.\n 2 Dallas  2010     4  4791 1005356810 160100    24290       6.2 2010.\n 3 Dallas  2010     6  4658 1055526309 166700    26812       6.7 2010.\n 4 Dallas  2010     3  4082  824662535 156800    24125       6.3 2010.\n 5 Dallas  2010     8  3476  738748272 164900    27346       7.2 2011.\n 6 Dallas  2010     7  3363  782294206 167800    27709       7.2 2010.\n 7 Dallas  2010    12  3215  743761990 161900    22557       6.4 2011.\n 8 Dallas  2010     9  3151  651529718 155200    26745       7.2 2011.\n 9 Dallas  2010    10  2977  606133350 154100    25688       7.1 2011.\n10 Dallas  2010    11  2795  608685059 154900    24254       6.8 2011.\n11 Dallas  2010     2  2774  548903564 149800    22991       6.1 2010.\n12 Dallas  2010     1  2210  420140118 140800    21825       5.7 2010 \n\n\nSometimes metrics like home sales have a seasonal quality to them, much like retail sales. There are times of year that are naturally more active, and others where it’s slower. That means sometimes you’ll want to isolate the same time period across multiple years to compare how it went based on similar times of year. Let’s do that here: we’re going to look only for sales that happened in the month of JUNE. Filter for the city is Dallas, and the month is June (the number 6). Put in the housesale dataset title followed by a pipe then filter by the column city with double equals sign for “Dallas”. Separate with a comma and filter by the column month for “6”.\n\n\nCode\nhousesales %>%\n  filter(city == \"Dallas\" , month == 6)\n\n\n# A tibble: 16 × 9\n   city    year month sales     volume median listings inventory  date\n   <chr>  <int> <int> <dbl>      <dbl>  <dbl>    <dbl>     <dbl> <dbl>\n 1 Dallas  2000     6  4738  846254912 138800    14392       3.8 2000.\n 2 Dallas  2001     6  5065  941310414 146300    18372       4.8 2001.\n 3 Dallas  2002     6  4423  817899695 148400    22043       5.6 2002.\n 4 Dallas  2003     6  4847  920352540 154000    27169       6.9 2003.\n 5 Dallas  2004     6  5511 1134082858 159300    28725       6.6 2004.\n 6 Dallas  2005     6  6416 1352136392 158900    28875       6.1 2005.\n 7 Dallas  2006     6  7117 1546598624 163400    30323       5.8 2006.\n 8 Dallas  2007     6  6207 1432055051 169100    33624       6.4 2007.\n 9 Dallas  2008     6  5180 1134394581 164400    30340       6.6 2008.\n10 Dallas  2009     6  4691 1001824336 166600    24677       6.5 2009.\n11 Dallas  2010     6  4658 1055526309 166700    26812       6.7 2010.\n12 Dallas  2011     6  4541 1009183879 165700    23968       7.2 2011.\n13 Dallas  2012     6  5196 1209024869 177900    17587       4.6 2012.\n14 Dallas  2013     6  5981 1593190134 202800    13339       2.9 2013.\n15 Dallas  2014     6  6309 1703805588 217100    12432       2.5 2014.\n16 Dallas  2015     6  6532 1960556658 242300    11014       2.2 2015.\n\n\nBuild on what we’ve done above. Now that we have June numbers for Dallas for every year, arrange the results by sales from highest-to-lowest to show what year had the highest June sales. Put in the housesale dataset title followed by a pipe then filter by the column city with double equals sign for “Dallas”. Separate with a comma and then filter by the column month for “6”, followed by a pipe. Use the arrange command with (desc) in parentheses to arrange by the sales column.\n\n\nCode\nhousesales %>%\n  filter(city == \"Dallas\" , month == 6) %>%\n  arrange(desc(sales))\n\n\n# A tibble: 16 × 9\n   city    year month sales     volume median listings inventory  date\n   <chr>  <int> <int> <dbl>      <dbl>  <dbl>    <dbl>     <dbl> <dbl>\n 1 Dallas  2006     6  7117 1546598624 163400    30323       5.8 2006.\n 2 Dallas  2015     6  6532 1960556658 242300    11014       2.2 2015.\n 3 Dallas  2005     6  6416 1352136392 158900    28875       6.1 2005.\n 4 Dallas  2014     6  6309 1703805588 217100    12432       2.5 2014.\n 5 Dallas  2007     6  6207 1432055051 169100    33624       6.4 2007.\n 6 Dallas  2013     6  5981 1593190134 202800    13339       2.9 2013.\n 7 Dallas  2004     6  5511 1134082858 159300    28725       6.6 2004.\n 8 Dallas  2012     6  5196 1209024869 177900    17587       4.6 2012.\n 9 Dallas  2008     6  5180 1134394581 164400    30340       6.6 2008.\n10 Dallas  2001     6  5065  941310414 146300    18372       4.8 2001.\n11 Dallas  2003     6  4847  920352540 154000    27169       6.9 2003.\n12 Dallas  2000     6  4738  846254912 138800    14392       3.8 2000.\n13 Dallas  2009     6  4691 1001824336 166600    24677       6.5 2009.\n14 Dallas  2010     6  4658 1055526309 166700    26812       6.7 2010.\n15 Dallas  2011     6  4541 1009183879 165700    23968       7.2 2011.\n16 Dallas  2002     6  4423  817899695 148400    22043       5.6 2002.\n\n\nNow do the same as before, but this time instead of Dallas, show results for Corpus Christi. Put in the housesale dataset title followed by a pipe then filter by the column city with double equals sign for “Corpus Christi”. Separate with a comma and then filter by the column month for “6”, followed by a pipe. Use the arrange command with (desc) in parentheses to arrange by the sales column.\n\n\nCode\nhousesales %>%\n  filter(city == \"Corpus Christi\" , month == 6) %>%\n  arrange(desc(sales))\n\n\n# A tibble: 16 × 9\n   city            year month sales    volume median listings inventory  date\n   <chr>          <int> <int> <dbl>     <dbl>  <dbl>    <dbl>     <dbl> <dbl>\n 1 Corpus Christi  2006     6   565  88945000 136300     2727       6.3 2006.\n 2 Corpus Christi  2005     6   527  81105000 129300     1783       4.5 2005.\n 3 Corpus Christi  2015     6   493 104904076 189600     2133       5.4 2015.\n 4 Corpus Christi  2014     6   483  95154634 170800     1948       5.1 2014.\n 5 Corpus Christi  2004     6   468  65745000 118600     1751       4.6 2004.\n 6 Corpus Christi  2013     6   454  84759840 160500     1901       5.2 2013.\n 7 Corpus Christi  2012     6   437  77838239 144600     2095       6.6 2012.\n 8 Corpus Christi  2007     6   433  72881251 143200     2425       6   2007.\n 9 Corpus Christi  2003     6   400  52690000 107000     1664       5.1 2003.\n10 Corpus Christi  2008     6   393  65130000 143500     3150       9.1 2008.\n11 Corpus Christi  2001     6   384  42100000  91200     2482       8.4 2001.\n12 Corpus Christi  2009     6   372  59297003 140000     2967      10.4 2009.\n13 Corpus Christi  2010     6   361  56136687 143000     3085      10.5 2010.\n14 Corpus Christi  2011     6   347  54172618 139700     2904      10.7 2011.\n15 Corpus Christi  2000     6   339  37750000  93600     2632       9.6 2000.\n16 Corpus Christi  2002     6   332  38020000  95800     1467       5   2002."
  }
]